{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c81a55-1b24-4c02-8429-56d0729dc85f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Creating an NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d14ad5d-8b24-4ed7-bf41-99a74e585029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import threading\n",
    "import queue\n",
    "import feedparser\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f6a5d28-3ea0-464a-9ee5-9dfdeccc92b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = []\n",
    "queues = [queue.Queue(), queue.Queue()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec0196e7-2a84-4800-8348-085ac185d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWords():\n",
    "    url = 'https://timesofindia.indiatimes.com/rssfeeds/1081479906.cms'\n",
    "    feed = feedparser.parse(url)\n",
    "    for entry in feed['entries'][:5]:\n",
    "        text = entry['title']\n",
    "        if 'ex' in text:\n",
    "            continue\n",
    "        words = nltk.word_tokenize(text)\n",
    "        data = {'uuid': uuid.uuid4(), 'input': words}\n",
    "        queues[0].put(data, True)\n",
    "        print(\">> {} : {}\".format(data[\"uuid\"], text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5f9a67c-95bc-48f7-9ef0-d93069f9c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPOS():\n",
    "    while True:\n",
    "        if queues[0].empty():\n",
    "            break\n",
    "        else:\n",
    "            data = queues[0].get()\n",
    "            words = data['input']\n",
    "            postags = nltk.pos_tag(words)\n",
    "            queues[0].task_done()\n",
    "            queues[1].put({'uuid': data['uuid'], 'input': postags}, True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90360ea7-9344-4ddf-95b4-f5147caafafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractNE():\n",
    "    while True:\n",
    "        if queues[1].empty():\n",
    "            break\n",
    "        else:\n",
    "            data = queues[1].get()\n",
    "            postags = data['input']\n",
    "            queues[1].task_done()\n",
    "            chunks = nltk.ne_chunk(postags, binary = False)\n",
    "            print(\" << {} : \".format(data['uuid']), end = '')\n",
    "            for path in chunks:\n",
    "                try:\n",
    "                    label = path.label()\n",
    "                    print(path, end = ',')\n",
    "                except:\n",
    "                    pass\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "114c514f-eba1-4e95-ad15-11a047d60bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 7c6ac89e-beb6-4001-b51d-4591f5634528 : How much fanaticism is too much? - #BigStory\n",
      ">> 2a9e7866-b8d2-4810-b282-2ead4c6c2730 : Live: Phone Bhoot, Mili, Double XL BO Day 1\n",
      ">> 8c9805c2-2d70-4ecc-8836-3897140dd09f : Bomma Blockbuster- Review: 2/5\n",
      ">> c6acd8c5-26b0-4b5e-94f1-1ab7adb698b6 : Actresses who gained weight for films\n",
      ">> 453e6313-34ac-4a79-826b-10cbefbb9707 : Movie Review: 'Mili'- 2.5/5\n"
     ]
    }
   ],
   "source": [
    "e = threading.Thread(target = extractWords())\n",
    "e.start()\n",
    "threads.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d257fd7-6c50-4d74-b547-80847cc61c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = threading.Thread(target = extractPOS())\n",
    "p.start()\n",
    "threads.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89d6be1e-698a-48a6-9834-d6cd26879889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " << 7c6ac89e-beb6-4001-b51d-4591f5634528 : \n",
      " << 2a9e7866-b8d2-4810-b282-2ead4c6c2730 : (PERSON Phone/NN Bhoot/NNP),(PERSON Mili/NNP),(PERSON Double/NNP XL/NNP BO/NNP),\n",
      " << 8c9805c2-2d70-4ecc-8836-3897140dd09f : (PERSON Bomma/NNP),\n",
      " << c6acd8c5-26b0-4b5e-94f1-1ab7adb698b6 : \n",
      " << 453e6313-34ac-4a79-826b-10cbefbb9707 : (PERSON Movie/NNP),\n"
     ]
    }
   ],
   "source": [
    "n = threading.Thread(target = extractNE())\n",
    "n.start()\n",
    "threads.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62270405-4a15-4e6a-94ee-6179f9891e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "queues[0].join()\n",
    "queues[1].join()\n",
    "\n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b313da04-6259-48c9-b280-3f664ee3c081",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Solving the text similarity problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "974e4ccf-4f32-4da1-98d8-8747a778a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29615a4d-e75d-4c5d-92b5-9116fefb9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSimilarityExample:\n",
    "    def __init__(self):\n",
    "        self.statements = [\n",
    "            'ruled india',\n",
    "            'Chalukyas ruled Badami',\n",
    "            'So many kingdoms ruled India',\n",
    "            'Lalbagh is a botanical garden in India'\n",
    "        ]\n",
    "    def TF(self, sentence):\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        freq = nltk.FreqDist(words)\n",
    "        dictionary = {}\n",
    "        for key in freq.keys():\n",
    "            norm = freq[key]/float(len(words))\n",
    "            dictionary[key] = norm\n",
    "        return dictionary\n",
    "    \n",
    "    def IDF(self):\n",
    "        def idf(TotalNumberOfDocuments, NumberOfDocumentsWithThisWord):\n",
    "            return 1.0 + math.log(TotalNumberOfDocuments/NumberOfDocumentsWithThisWord)\n",
    "        numDocuments = len(self.statements)\n",
    "        uniqueWords = {}\n",
    "        idfValues = {}\n",
    "        for sentence in self.statements:\n",
    "            for word in nltk.word_tokenize(sentence.lower()):\n",
    "                if word not in uniqueWords:\n",
    "                    uniqueWords[word] = 1\n",
    "                else:\n",
    "                    uniqueWords[word] += 1\n",
    "        for word in uniqueWords:\n",
    "            idfValues[word] = idf(numDocuments, uniqueWords[word])\n",
    "        return idfValues\n",
    "    \n",
    "    def TF_IDF(self, query):\n",
    "        words = nltk.word_tokenize(query.lower())\n",
    "        idf = self.IDF()\n",
    "        vectors = {}\n",
    "        for sentence in self.statements:\n",
    "            tf = self.TF(sentence)\n",
    "            for word in words:\n",
    "                tfv = tf[word] if word in tf else 0.0\n",
    "                idfv = idf[word] if word in idf else 0.0\n",
    "                mul = tfv * idfv\n",
    "                if word not in vectors:\n",
    "                    vectors[word] = []\n",
    "                vectors[word].append(mul)\n",
    "        return vectors\n",
    "    \n",
    "    def displayVectors(self, vectors):\n",
    "        print(self.statements)\n",
    "        for word in vectors:\n",
    "            print(\"{} -> {}\".format(word, vectors[word]))\n",
    "            \n",
    "    def cosineSimilarity(self):\n",
    "        vec = TfidfVectorizer()\n",
    "        matrix = vec.fit_transform(self.statements)\n",
    "        for j in range(1, 5):\n",
    "            i = j - 1\n",
    "            print(\"\\tsimilarity of document {} with others\".format(i))\n",
    "            similarity = cosine_similarity(matrix[i:j], matrix)\n",
    "            print(similarity)\n",
    "    \n",
    "    def demo(self):\n",
    "        inputQuery = self.statements[0]\n",
    "        vectors = self.TF_IDF(inputQuery)\n",
    "        self.displayVectors(vectors)\n",
    "        self.cosineSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6b96b06-1e21-458c-b8ec-33184c15e450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ruled india', 'Chalukyas ruled Badami', 'So many kingdoms ruled India', 'Lalbagh is a botanical garden in India']\n",
      "ruled -> [0.6438410362258904, 0.42922735748392693, 0.2575364144903562, 0.0]\n",
      "india -> [0.6438410362258904, 0.0, 0.2575364144903562, 0.18395458177882582]\n",
      "\tsimilarity of document 0 with others\n",
      "[[1.         0.29088811 0.46216171 0.19409143]]\n",
      "\tsimilarity of document 1 with others\n",
      "[[0.29088811 1.         0.13443735 0.        ]]\n",
      "\tsimilarity of document 2 with others\n",
      "[[0.46216171 0.13443735 1.         0.08970163]]\n",
      "\tsimilarity of document 3 with others\n",
      "[[0.19409143 0.         0.08970163 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity = TextSimilarityExample()\n",
    "similarity.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb862c-70b4-4613-b3ce-433f1bd589af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Identifying topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6291ff2-87cb-454d-a297-01a7bc9f9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "434efad4-c595-4c84-9977-e66bdc487017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentifyingTopicExample:\n",
    "    def getDocuments(self):\n",
    "        url = 'https://sports.yahoo.com/mlb/rss.xml'\n",
    "        feed = feedparser.parse(url)\n",
    "        self.documents = []\n",
    "        for entry in feed['entries'][:5]:\n",
    "            text = entry['title']\n",
    "            if 'ex' in text:\n",
    "                continue\n",
    "            self.documents.append(text)\n",
    "            print(\"-- {}\".format(text))\n",
    "        print(\"INFO: Fetchng documents from {} completed\".format(url))\n",
    "\n",
    "    def cleanDocuments(self):\n",
    "        tokenizer = RegexpTokenizer(r'[a-zA-z]+')\n",
    "        en_stop = set(stopwords.words('english'))\n",
    "        self.cleaned = []\n",
    "        for doc in self.documents:\n",
    "            lowercase = doc.lower()\n",
    "            words = tokenizer.tokenize(lowercase)\n",
    "            non_stop_words = [i for i in words if not i in en_stop]\n",
    "            self.cleaned.append(non_stop_words)\n",
    "    \n",
    "    def doLDA(self):\n",
    "        dictionary = corpora.Dictionary(self.cleaned)\n",
    "        corpus = [dictionary.doc2bow(cleandoc) for cleandoc in self.cleaned]\n",
    "        ldamodel = models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary)\n",
    "        print(ldamodel.print_topics(num_topics=2, num_words=4))\n",
    "        \n",
    "    def run(self):\n",
    "        self.getDocuments()\n",
    "        self.cleanDocuments()\n",
    "        self.doLDA()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c4a0ac2-1cea-49b0-b308-ce8d82aea07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Verlander's first win in World Series wasn't easy\n",
      "-- Detroit Tigers LHP Joey Wentz is breakout candidate for 2023 after strong finish\n",
      "-- Yanksâ€™ Cashman plans to push ahead on Aaron Judge talks\n",
      "-- Will free-agent starter Chris Bassitt be one and done with the Mets? | Baseball Night in NY\n",
      "INFO: Fetchng documents from https://sports.yahoo.com/mlb/rss.xml completed\n",
      "[(0, '0.050*\"lhp\" + 0.050*\"detroit\" + 0.050*\"breakout\" + 0.050*\"finish\"'), (1, '0.038*\"one\" + 0.038*\"done\" + 0.038*\"ny\" + 0.038*\"chris\"')]\n"
     ]
    }
   ],
   "source": [
    "topicExample = IdentifyingTopicExample()\n",
    "topicExample.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb46b78-2a6d-479a-8439-17fb1e168afd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Resolving anaphora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65bf0a7c-3f6a-4abe-8b87-1ea29f579e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.corpus import names\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65be15f8-18c8-4e4a-9b57-3bb64e822c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnaphoraExample:\n",
    "    def __init__(self):\n",
    "        males = [(name, 'male') for name in names.words('male.txt')]\n",
    "        females = [(name, 'female') for name in names.words('female.txt')]\n",
    "        combined = males + females\n",
    "        random.shuffle(combined)\n",
    "        training = [(self.feature(name), gender) for (name, gender) in combined]\n",
    "        self._classifier = nltk.NaiveBayesClassifier.train(training)\n",
    "        \n",
    "    def feature(self,word):\n",
    "        return {'last(1)' : word[-1]}\n",
    "    \n",
    "    def gender(self, word):\n",
    "        return self._classifier.classify(self.feature(word))\n",
    "    \n",
    "    def learnAnaphora(self):\n",
    "        sentences = [\n",
    "            \"John is a man. He walks\",\n",
    "            \"John and Mary are married. They have two kids\",\n",
    "            \"In order for Ravi to be successful, he should follow John\",\n",
    "            \"John met Mary in Barista. She asked him to order a Pizza\"\n",
    "        ]\n",
    "        \n",
    "        for sent in sentences:\n",
    "            chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent)), binary = False)\n",
    "            stack = []\n",
    "            print(sent)\n",
    "            items = tree2conlltags(chunks)\n",
    "            for item in items:\n",
    "                if item[1] == 'NNP' and (item[2] == 'B-PERSON' or item[2] == 'O'):\n",
    "                    stack.append((item[0], self.gender(item[0])))\n",
    "                elif item[1] == 'CC':\n",
    "                    stack.append(item[0])\n",
    "                elif item[1] == 'PRP':\n",
    "                    stack.append(item[0])   \n",
    "            print(\"\\t {}\".format(stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5035202f-2560-415d-8027-e9574af92652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John is a man. He walks\n",
      "\t [('John', 'male'), 'He']\n",
      "John and Mary are married. They have two kids\n",
      "\t [('John', 'male'), 'and', ('Mary', 'female'), 'They']\n",
      "In order for Ravi to be successful, he should follow John\n",
      "\t [('Ravi', 'female'), 'he', ('John', 'male')]\n",
      "John met Mary in Barista. She asked him to order a Pizza\n",
      "\t [('John', 'male'), ('Mary', 'female'), 'She', 'him']\n"
     ]
    }
   ],
   "source": [
    "anaphora = AnaphoraExample()\n",
    "anaphora.learnAnaphora()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cee859-785c-44bb-bc8e-d722d6ce6ef3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Disambiguating word sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8fdc05f0-7242-4335-873f-3550c665b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def understandWordSenseExample():\n",
    "    words = ['wind', 'date', 'left']\n",
    "    print(\"--examples--\")\n",
    "    for word in words:\n",
    "        syns = nltk.corpus.wordnet.synsets(word)\n",
    "        for syn in syns[:2]:\n",
    "            for example in syn.examples()[:2]:\n",
    "                print(\"{} -> {} -> {}\".format(word, syn.name(), example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a22b6608-2eeb-48c7-bdc0-0c56ecf146d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def understandBuiltinWSD():\n",
    "    print(\"--built in wsd\")\n",
    "    maps = [\n",
    "        ('Is it the fish net that you are using to catch fish ?', 'fish',\n",
    "        'n'),\n",
    "        ('Please dont point your finger at others.', 'point', 'n'),\n",
    "        ('I went to the river bank to see the sun rise', 'bank', 'n'),\n",
    "    ]\n",
    "    for m in maps:\n",
    "        print(\"sense '{}' for '{}' -> '{}'\".format(m[0], m[1],nltk.wsd.lesk(m[0], m[1], m[2])))\n",
    "        nltk.wsd.lesk(m[0], m[1], m[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0df02df3-9219-4f9c-adde-110bc5930232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--examples--\n",
      "wind -> wind.n.01 -> trees bent under the fierce winds\n",
      "wind -> wind.n.01 -> when there is no wind, row\n",
      "wind -> wind.n.02 -> the winds of change\n",
      "date -> date.n.01 -> what is the date today?\n",
      "date -> date.n.02 -> his date never stopped talking\n",
      "left -> left.n.01 -> she stood on the left\n",
      "--built in wsd\n",
      "sense 'Is it the fish net that you are using to catch fish ?' for 'fish' -> 'Synset('pisces.n.02')'\n",
      "sense 'Please dont point your finger at others.' for 'point' -> 'Synset('point.n.25')'\n",
      "sense 'I went to the river bank to see the sun rise' for 'bank' -> 'Synset('savings_bank.n.02')'\n"
     ]
    }
   ],
   "source": [
    "understandWordSenseExample()\n",
    "understandBuiltinWSD()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb15254-bd40-4902-aa23-f6b2f345f41a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Performing sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cb6e1e1d-9764-47c5-8cfe-7abafd33ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.sentiment.util\n",
    "import nltk.sentiment.sentiment_analyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6f76b2d-3e77-42dd-b15c-594ccf25f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mySentimentAnalyzer():\n",
    "    def score_feedback(text):\n",
    "        positive_words = ['love', 'genuine', 'liked']\n",
    "        if '_NEG' in ''.join(nltk.sentiment.util.mark_negation(text.split())):\n",
    "            score = -1\n",
    "        else:\n",
    "            analysis = nltk.sentiment.util.extract_unigram_feats(text.split(), positive_words)\n",
    "            if True in analysis.values():\n",
    "                score = 1\n",
    "            else:\n",
    "                score = 0\n",
    "        return score\n",
    "    \n",
    "    feedback = \"\"\"I love the items in this shop, very genuine and quality is well maintained. \n",
    "    I have visited this shop and had samosa, my friends liked it very much. ok average food in this shop. \n",
    "    Fridays are very busy in this shop, do not place orders during this day.\"\"\"\n",
    "        \n",
    "    print(' -- custom scorer --')\n",
    "    for text in feedback.split(\"\\n\"):\n",
    "        print(\"score = {} for >> {}\".format(score_feedback(text), text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c046535a-f8f6-48da-a8bf-d06188cb49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advancedSentimentAnalyzer():\n",
    "    sentences = [\n",
    "        ':)',\n",
    "        ':(',\n",
    "        'She is so :(',\n",
    "        'I love the way cricket is played by the champions',\n",
    "        'She neither likes coffee nor tea',\n",
    "    ]\n",
    "    senti = SentimentIntensityAnalyzer()\n",
    "    print(' -- built-in intensity analyser --')\n",
    "    for sentence in sentences:\n",
    "        print('[{}]'.format(sentence), end=' --> ')\n",
    "        kvp = senti.polarity_scores(sentence)\n",
    "        for k in kvp:\n",
    "            print('{} = {}, '.format(k, kvp[k]), end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "064a28e2-8577-44ec-a1d7-d54278b5bd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- built-in intensity analyser --\n",
      "[:)] --> neg = 0.0, neu = 0.0, pos = 1.0, compound = 0.4588, \n",
      "[:(] --> neg = 1.0, neu = 0.0, pos = 0.0, compound = -0.4404, \n",
      "[She is so :(] --> neg = 0.555, neu = 0.445, pos = 0.0, compound = -0.5777, \n",
      "[I love the way cricket is played by the champions] --> neg = 0.0, neu = 0.375, pos = 0.625, compound = 0.875, \n",
      "[She neither likes coffee nor tea] --> neg = 0.318, neu = 0.682, pos = 0.0, compound = -0.3252, \n",
      " -- custom scorer --\n",
      "score = 1 for >> I love the items in this shop, very genuine and quality is well maintained. \n",
      "score = 1 for >>     I have visited this shop and had samosa, my friends liked it very much. ok average food in this shop. \n",
      "score = -1 for >>     Fridays are very busy in this shop, do not place orders during this day.\n"
     ]
    }
   ],
   "source": [
    "advancedSentimentAnalyzer()\n",
    "mySentimentAnalyzer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfb361e-68f7-46a9-8cf7-2835502dc39a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Creating a conversational assistant or chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "52339da4-cbd8-418f-8441-1c1db2a68ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def builtinEngines(whichOne):\n",
    "    if whichOne == 'eliza':\n",
    "        nltk.chat.eliza.demo()\n",
    "    elif whichOne == 'iesha':\n",
    "        nltk.chat.iesha.demo()\n",
    "    elif whichOne == 'rude':\n",
    "        nltk.chat.rude.demo()\n",
    "    elif whichOne == 'suntsu':\n",
    "        nltk.chat.suntsu.demo()\n",
    "    elif whichOne == 'zen':\n",
    "        nltk.chat.zen.demo()\n",
    "    else:\n",
    "        print(\"unknown built-in chat engine {}\".format(whichOne))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "02dbf53a-a78e-4ace-b58d-11dec157006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myEngine():\n",
    "    chatpairs = (\n",
    "        (r\"(.*?)Stock price(.*)\",\n",
    "        (\"Today stock price is 100\",\n",
    "        \"I am unable to find out the stock price.\")),\n",
    "        (r\"(.*?)not well(.*)\",\n",
    "        (\"Oh, take care. May be you should visit a doctor\",\n",
    "        \"Did you take some medicine ?\")),\n",
    "        (r\"(.*?)raining(.*)\",\n",
    "        (\"Its monsoon season, what more do you expect ?\",\n",
    "        \"Yes, its good for farmers\")),\n",
    "        (r\"How(.*?)health(.*)\",\n",
    "        (\"I am always healthy.\",\n",
    "        \"I am a program, super healthy!\")),\n",
    "        (r\".*\",\n",
    "        (\"I am good. How are you today ?\",\n",
    "        \"What brings you here ?\"))\n",
    "    )\n",
    "    def chat():\n",
    "        print(\"!\"*80)\n",
    "        print(\" >> my Engine << \")\n",
    "        print(\"Talk to the program using normal english\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"Enter 'quit' when done\")\n",
    "        chatbot = nltk.chat.util.Chat(chatpairs,nltk.chat.util.reflections)\n",
    "        chatbot.converse()\n",
    "        \n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482bac82-ae19-41c9-9f91-b39ef5977d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== demo of eliza ===\n",
      "Therapist\n",
      "---------\n",
      "Talk to the program by typing in plain English, using normal upper-\n",
      "and lower-case letters and punctuation.  Enter \"quit\" when done.\n",
      "========================================================================\n",
      "Hello.  How are you feeling today?\n"
     ]
    }
   ],
   "source": [
    "for engine in ['eliza', 'iesha', 'rude', 'suntsu', 'zen']:\n",
    "    print(\"=== demo of {} ===\".format(engine))\n",
    "    builtinEngines(engine)\n",
    "    print()\n",
    "myEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73273b9e-aba7-4780-b870-c8729d1b20eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
